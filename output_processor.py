"""
Output processor for Bloat Axis GEPA.

Generates SGLang-compatible CustomLogitProcessor classes from learned
bloat axis configurations.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

try:
    from .gepa_types import BloatAxis, Candidate, ConditionType
except ImportError:
    from gepa_types import BloatAxis, Candidate, ConditionType

logger = logging.getLogger(__name__)


# Template for the generated logit processor
LOGIT_PROCESSOR_TEMPLATE = '''"""
Learned Bloat Axis Logit Processor

Generated by Bloat Axis GEPA on {timestamp}

This processor applies learned penalties to reduce verbosity in worker responses
while maintaining correctness.

Usage with SGLang:
    from learned_logit_processor import LearnedBloatAxisProcessor
    
    # In your generate call:
    extra_body={{
        "custom_logit_processor": LearnedBloatAxisProcessor().to_str(),
        "custom_params": LearnedBloatAxisProcessor.get_default_params(),
    }}
"""

import torch
from sglang.srt.sampling.custom_logit_processor import CustomLogitProcessor


class LearnedBloatAxisProcessor(CustomLogitProcessor):
    """
    SGLang-compatible logit processor with learned bloat-axis penalties.
    
    Discovered Axes:
{axes_documentation}
    
    Configuration:
        - min_new_tokens: {min_new_tokens}
        - total_axes: {num_axes}
    """
    
    # Learned configuration
    AXES_CONFIG = {axes_config_json}
    
    # Pre-computed token ID sets by condition type
    EARLY_PHASE_TOKEN_IDS = {early_phase_ids}
    ALWAYS_TOKEN_IDS = {always_ids}
    AFTER_NEWLINE_TOKEN_IDS = {after_newline_ids}
    WHITELIST_TOKEN_IDS = {whitelist_ids}
    
    # Learned penalties
    EARLY_PHASE_PENALTY = {early_phase_penalty}
    ALWAYS_PENALTY = {always_penalty}
    AFTER_NEWLINE_PENALTY = {after_newline_penalty}
    
    # Anti-truncation settings
    MIN_NEW_TOKENS = {min_new_tokens}
    EOS_BONUS_AFTER_MIN = 1.2
    INTRO_WINDOW = 48
    
    @classmethod
    def get_default_params(cls) -> dict:
        """Get default custom_params for this processor."""
        return {{
            "eos_token_id": 128001,  # Llama-3 default
            "min_new_tokens": cls.MIN_NEW_TOKENS,
            "current_len": 0,
            "eos_bonus_after_min": cls.EOS_BONUS_AFTER_MIN,
            "intro_window": cls.INTRO_WINDOW,
            "intro_penalty": cls.EARLY_PHASE_PENALTY,
            "always_penalty": cls.ALWAYS_PENALTY,
            "list_penalty": cls.AFTER_NEWLINE_PENALTY,
            "intro_token_ids": list(cls.EARLY_PHASE_TOKEN_IDS),
            "always_token_ids": list(cls.ALWAYS_TOKEN_IDS),
            "list_token_ids": list(cls.AFTER_NEWLINE_TOKEN_IDS),
            "whitelist_token_ids": list(cls.WHITELIST_TOKEN_IDS),
        }}
    
    def __call__(self, logits, custom_param_list):
        """
        Apply learned bloat-axis penalties at each decoding step.
        
        Args:
            logits: (batch_size, vocab_size) tensor of next-token logits
            custom_param_list: List of dicts with per-request parameters
        """
        assert logits.shape[0] == len(custom_param_list)
        
        for i, p in enumerate(custom_param_list):
            # Extract parameters
            eos_token_id = int(p.get("eos_token_id", 128001))
            min_new_tokens = int(p.get("min_new_tokens", self.MIN_NEW_TOKENS))
            current_len = int(p.get("current_len", 0))
            
            # Penalty values
            eos_bonus_after_min = float(p.get("eos_bonus_after_min", self.EOS_BONUS_AFTER_MIN))
            intro_window = int(p.get("intro_window", self.INTRO_WINDOW))
            intro_penalty = float(p.get("intro_penalty", self.EARLY_PHASE_PENALTY))
            always_penalty = float(p.get("always_penalty", self.ALWAYS_PENALTY))
            list_penalty = float(p.get("list_penalty", self.AFTER_NEWLINE_PENALTY))
            
            # Token ID sets (use class defaults if not provided in params)
            intro_token_ids = set(p.get("intro_token_ids", list(self.EARLY_PHASE_TOKEN_IDS)))
            always_token_ids = set(p.get("always_token_ids", list(self.ALWAYS_TOKEN_IDS)))
            list_token_ids = set(p.get("list_token_ids", list(self.AFTER_NEWLINE_TOKEN_IDS)))
            whitelist_ids = set(p.get("whitelist_token_ids", list(self.WHITELIST_TOKEN_IDS)))
            
            newline_token_ids = p.get("newline_token_ids", [198, 628])  # Common newline tokens
            last_token_id = p.get("last_token_id", None)
            
            # 1) Anti-truncation: block EOS before min_new_tokens
            if current_len < min_new_tokens:
                logits[i, eos_token_id] = -1e9
            else:
                logits[i, eos_token_id] += eos_bonus_after_min
            
            # 2) Always-on penalties (respecting whitelist)
            if always_token_ids and always_penalty > 0:
                for tid in always_token_ids:
                    if tid not in whitelist_ids:
                        logits[i, tid] -= always_penalty
            
            # 3) Early-phase penalties (preamble/intro suppression)
            if current_len < intro_window and intro_token_ids and intro_penalty > 0:
                for tid in intro_token_ids:
                    if tid not in whitelist_ids:
                        logits[i, tid] -= intro_penalty
            
            # 4) After-newline penalties (list markers, etc.)
            if list_token_ids and list_penalty > 0:
                for tid in list_token_ids:
                    if tid not in whitelist_ids:
                        logits[i, tid] -= list_penalty
            
            # 5) Extra penalty after actual newlines
            if last_token_id is not None and int(last_token_id) in newline_token_ids:
                for tid in list_token_ids:
                    if tid not in whitelist_ids:
                        logits[i, tid] -= (list_penalty + 1.0)
        
        return logits


# Standalone configuration export
AXES_CONFIG = LearnedBloatAxisProcessor.AXES_CONFIG
'''


def generate_logit_processor(
    candidate: Candidate,
    output_path: Optional[Path] = None,
) -> str:
    """
    Generate a Python file with the learned logit processor.
    
    Args:
        candidate: The best candidate from evolution
        output_path: Path to write the file (optional)
        
    Returns:
        Generated Python code as string
    """
    # Collect token IDs by condition type
    early_phase_ids: Set[int] = set()
    always_ids: Set[int] = set()
    after_newline_ids: Set[int] = set()
    
    early_phase_penalties: List[float] = []
    always_penalties: List[float] = []
    after_newline_penalties: List[float] = []
    
    for axis in candidate.axes:
        if axis.condition_type == ConditionType.EARLY_PHASE:
            early_phase_ids |= axis.token_ids
            early_phase_penalties.append(axis.penalty)
        elif axis.condition_type == ConditionType.ALWAYS:
            always_ids |= axis.token_ids
            always_penalties.append(axis.penalty)
        elif axis.condition_type == ConditionType.AFTER_NEWLINE:
            after_newline_ids |= axis.token_ids
            after_newline_penalties.append(axis.penalty)
    
    # Compute average penalties per category
    early_phase_penalty = sum(early_phase_penalties) / len(early_phase_penalties) if early_phase_penalties else 5.0
    always_penalty = sum(always_penalties) / len(always_penalties) if always_penalties else 0.5
    after_newline_penalty = sum(after_newline_penalties) / len(after_newline_penalties) if after_newline_penalties else 4.0
    
    # Generate documentation
    axes_docs = []
    for i, axis in enumerate(candidate.axes, 1):
        cond_str = axis.condition_type.value
        if axis.condition_params:
            cond_str += f" ({axis.condition_params})"
        axes_docs.append(
            f"        {i}. {axis.name}: penalty={axis.penalty:.1f}, "
            f"condition={cond_str}, phrases={len(axis.phrases)}"
        )
    axes_documentation = "\n".join(axes_docs) if axes_docs else "        (no axes)"
    
    # Generate axes config JSON
    axes_config = [axis.to_dict() for axis in candidate.axes]
    axes_config_json = json.dumps(axes_config, indent=4)
    # Indent for Python
    axes_config_json = axes_config_json.replace("\n", "\n    ")
    
    # Format sets
    def format_set(s: Set[int]) -> str:
        if not s:
            return "set()"
        items = sorted(s)
        if len(items) <= 10:
            return "{" + ", ".join(str(x) for x in items) + "}"
        # For large sets, use set()
        return "set(" + str(items) + ")"
    
    # Fill template
    code = LOGIT_PROCESSOR_TEMPLATE.format(
        timestamp=datetime.now().isoformat(),
        axes_documentation=axes_documentation,
        min_new_tokens=candidate.min_new_tokens,
        num_axes=len(candidate.axes),
        axes_config_json=axes_config_json,
        early_phase_ids=format_set(early_phase_ids),
        always_ids=format_set(always_ids),
        after_newline_ids=format_set(after_newline_ids),
        whitelist_ids=format_set(candidate.whitelist),
        early_phase_penalty=early_phase_penalty,
        always_penalty=always_penalty,
        after_newline_penalty=after_newline_penalty,
    )
    
    # Write to file if path provided
    if output_path:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            f.write(code)
        
        logger.info(f"Generated logit processor at {output_path}")
    
    return code


def export_axes_config(
    candidate: Candidate,
    output_path: Path,
):
    """
    Export axes configuration as JSON.
    
    Args:
        candidate: Candidate to export
        output_path: Path to write JSON file
    """
    config = {
        "axes": [axis.to_dict() for axis in candidate.axes],
        "whitelist": list(candidate.whitelist),
        "min_new_tokens": candidate.min_new_tokens,
        "timestamp": datetime.now().isoformat(),
    }
    
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    logger.info(f"Exported axes config to {output_path}")


def load_axes_config(config_path: Path) -> Candidate:
    """
    Load a candidate from a JSON config file.
    
    Args:
        config_path: Path to JSON config
        
    Returns:
        Candidate loaded from config
    """
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    axes = [BloatAxis.from_dict(a) for a in config.get("axes", [])]
    whitelist = set(config.get("whitelist", []))
    min_new_tokens = config.get("min_new_tokens", 48)
    
    return Candidate(
        axes=axes,
        whitelist=whitelist,
        min_new_tokens=min_new_tokens,
    )


def generate_sglang_integration_example(
    output_path: Path,
):
    """
    Generate an example script showing how to use the learned processor with SGLang.
    
    Args:
        output_path: Path to write the example script
    """
    example_code = '''"""
Example: Using the learned bloat axis processor with SGLang.

This script shows how to integrate the learned logit processor with
an SGLang server for inference.
"""

import openai
from learned_logit_processor import LearnedBloatAxisProcessor


def main():
    # Initialize SGLang client (assuming server is running)
    client = openai.OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="EMPTY",  # SGLang doesn't require API key
    )
    
    # Get the processor and params
    processor = LearnedBloatAxisProcessor()
    custom_params = LearnedBloatAxisProcessor.get_default_params()
    
    # Build the prompt
    prompt = """You are a helpful assistant answering questions about financial documents.
    
    Question: What was the company's revenue in FY2022?
    
    Please provide a concise answer based on the document."""
    
    # Make request with custom logit processor
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500,
        temperature=0.2,
        extra_body={
            "custom_logit_processor": processor.to_str(),
            "custom_params": custom_params,
        },
    )
    
    print("Response:", response.choices[0].message.content)


if __name__ == "__main__":
    main()
'''
    
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        f.write(example_code)
    
    logger.info(f"Generated integration example at {output_path}")
